\section{引言}

从人脸识别~\cite{Face-Recognition}、个性化推荐~\cite{Personal-Recommendation}、到智能家居~\cite{Smart-Home}、无人驾驶~\cite{Self-Driving}等应用领域，深度学习（Deep Learning, DL）~\cite{Deep-Learning}相关技术已经融入到社会的方方面面，为人类的生产生活带来了极大的便利。自然语言处理（Natural Language Processing, NLP）~\cite{NLP}作为深度学习领域的重要研究方向，长期以来备受研究人员关注。近年来，随着GPU算力的不断提升，各种语言模型也朝着更大参数量、更高准确度的方向迅猛发展。

大语言模型~\cite{LLM}（Large Language Models，LLM）是自然语言处理领域的一个分支。LLM通常拥有十亿级别，甚至万亿级别的参数量，因此需要海量文本数据进行训练。同时，LLM在多种类型的任务中展现出卓越性能，如文本摘要~\cite{Text-Summarization}、机器翻译~\cite{Machine-Translation}、代码生成~\cite{Code-Generation}、以及对话问答~\cite{Question-Answer}等，拥有巨大的科研潜力与商业价值。2021年GPT-3模型~\cite{Text-Summarization, GPT3}的问世标志着LLM研究领域的一个里程碑，自此，各大科研机构纷纷投入到相关研究中，各种LLM层出不穷，使得该领域的研究和应用热度空前高涨。

复杂的模型结构和庞大的参数量为LLM带来了卓越的应用效果，但却为LLM部署后的推理性能优化带来了极大挑战。特别地，LLM庞大的参数量导致推理过程中产生极高的内存占用。例如，GPT-175B模型仅在权重加载环节就需要消耗325GB的GPU内存空间~\cite{GPT-175B资源消耗}，需要使用至少5个NVIDIA A100 GPU（80GB），并引入复杂的推理并行化策略才能够完成模型推理。因此，如何降低LLM推理任务的内存资源占用对于LLM成功部署和推广至关重要。

传统的深度学习模型研究中提出了张量交换~\cite{Swapping}（Swapping）、张量重算~\cite{Recomputation}（Recomputation）等技术来降低推理过程中的显存占用。具体而言，张量交换技术通过张量生命周期分析、异步传输、动态调度等机制，将推理过程中不需要立即使用的张量从GPU内存交换到CPU内存；而张量重算则是将推理过程中的部分中间张量在不需要时释放，将计算图的关键节点保存下来，并在需要时重新计算这些中间结果。然而，简单地将张量交换和张量重算技术应用于LLM推理框架会导致以下两点不足：

首先，张量交换和张量重算技术虽然可以降低推理过程的GPU显存资源占用，但其对LLM推理性能的影响十分复杂，取决于服务器硬件配置（如GPU的计算能力，CPU-GPU双向传输带宽）、用户设置（如生成新token时的采样方式）、LLM任务类型与数据集选取、以及推理任务的运行时信息等。已有的LLM推理框架~\cite{Swapping, vLLM, ORCA}虽然已经集成了张量交换或张量重算技术，但其在GPU显存不足时只能固定选择上述技术中的一种，而无法根据上述信息选择更优者。\textcolor{red}{因此，寻找一种面对GPU内存不足时，能够在张量交换和张量重算间进行自适应选择，并进行合理调度的LLM推理服务框架，就显得尤为重要。}

% 另外，单请求延时和整体吞吐率在优化过程中存在矛盾。一部分传统工作，如FasterTransformer~\cite{FasterTransformer}等，以单请求延时为单一优化目标，将大部分集群资源分配给当前运行的少数请求，使得集群资源利用率降低，限制整体吞吐率；另一部分工作，如ORCA~\cite{vLLM, ORCA}等，以整体吞吐率为单一优化目标，通过提升批处理大小来增加资源利用率，但会导致单请求时延增加。整体吞吐率是面向服务器端的性能优化指标，体现了服务器端的处理效率。单请求平均延时是面向客户端的性能优化指标，体现了用户请求处理的实时性。在使用张量交换或张量重算技术优化LLM推理任务显存占用时，需要考虑其对推理任务整体吞吐率和单请求延时的影响，并在二者间进行权衡。

\textcolor{red}{然而，大部分传统工作在调度时忽略了公平性因素。例如在vLLM中，张量交换与张量重算拥有不同的优先级设置方案，导致不同用户请求的等待时间存在显著差异。这些工作大都以整体吞吐率作为性能优化指标，体现服务器端的处理效率。为了面向客户端体现请求间的调度公平性，本文引入平均带权周转时间作为度量指标，来反映用户请求的等待时间平均占比。综上所述，在使用张量交换或张量重算技术优化LLM推理任务显存占用时，需要考虑其对推理任务整体吞吐率和用户请求间公平性的影响，并在二者间进行分析权衡。}

为了解决上述缺陷，本文提出了AdaptiveLLM，一个基于张量交换和张量重算的自适应LLM推理服务框架。该框架实现了针对张量交换和张量重算的精准开销分析，并调用基于开销感知的内存优化策略和基于公平性的用户请求调度策略，在张量交换和张量重算技术间进行动态选择，在降低LLM推理显存占用的同时，降低其对LLM推理吞吐量和用户请求公平性的影响，进而实现LLM任务的高效推理。

具体而言，本文开展了以下工作：

\begin{itemize}

    \item \textbf{本文设计了一款张量重算开销分析器，实现了张量重算开销的精准预测。}通过算子粒度计算复杂度分析来识别张量重算开销的影响因素，而后建立回归预测模型预测单步推理执行时间。实验表明，张量重算开销分析器的预测误差在2\%以内。
    
    \item \textbf{本文设计了一款张量交换开销分析器，实现了张量交换开销的精准预测。}本文获取用户请求KV Cache的内存占用和GPU-CPU间通信效率信息，来对张量交换的数据传输开销进行预测。实验表明，张量交换开销分析器的预测误差在4\%以内。

    \item \textbf{本文设计和实现了一个基于张量交换和张量重算的自适应LLM推理服务框架AdaptiveLLM。}本文引入基于开销感知的内存优化策略，动态选择相应的内存优化技术，通过降低显存占用，来提升推理任务的整体吞吐率。\textcolor{red}{同时引入基于公平性的用户请求调度策略，降低带权周转时间。}
    
    \item \textbf{本文选择典型LLM模型和数据集对AdaptiveLLM进行全面实验评估。} 在典型LLM模型（OPT~\cite{OPT}、Llama~\cite{Llama}）和数据集（Chatbot~\cite{Chatbot}、Alpaca~\cite{Alpaca}、Summary~\cite{Summary}）上对AdaptiveLLM的张量重算开销分析器、张量交换开销分析器、内存优化决策器、和用户请求调度器模块的有效性进行了实验验证。结果表明，以vLLM和DeepSpeed作为基准框架时，AdaptiveLLM能够分别实现$1.3\times\sim2.1\times$和1.4$\times\sim2.3\times$的整体吞吐加速比，同时降低用户请求平均带权周转时间，缩减比例分别为$20\%\sim50\%$和$25\%\sim65\%$。

\end{itemize}
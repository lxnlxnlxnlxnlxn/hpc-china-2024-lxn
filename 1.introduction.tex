\section{引言}

从人脸识别、个性化推荐、到智能家居、无人驾驶等应用领域，深度学习（DL）相关技术已经融入到社会的方方面面，为人类的生产生活带来了极大的便利。自然语言处理（NLP）作为深度学习领域的重要研究方向，长期以来备受研究人员关注。近年来，随着GPU算力的不断提升，各种语言模型也朝着更大参数量、更高准确度的方向迅猛发展。

大语言模型~\cite{LLM}（Large Language Models，LLM）是自然语言处理领域的一个分支。LLM通常拥有极高的参数量，因此需要海量文本数据进行训练。同时，LLM在多种类型的任务中展现出卓越性能，如文本摘要、机器翻译、代码生成、以及对话问答等，拥有巨大的科研潜力与商业价值。2021年GPT-3模型~\cite{GPT3}的问世标志着LLM研究领域的一个里程碑，自此，各大科研机构纷纷投入到相关研究中，各种LLM层出不穷，使得该领域的研究和应用热度空前高涨。

复杂的模型结构和庞大的参数量为LLM带来了卓越的应用效果，但却为LLM部署后的推理性能优化带来了极大挑战。特别地，LLM庞大的参数量导致推理过程中产生极高的内存占用。例如，GPT-175B模型仅在权重加载环节就需要消耗325GB的GPU内存空间~\cite{GPT-175B资源消耗}，需要使用至少5个NVIDIA A100 GPU（80GB），并引入复杂的推理并行化策略才能够完成模型推理。因此，如何降低LLM推理任务的内存资源占用对于LLM成功部署和推广至关重要。

传统的深度学习模型研究中提出了张量交换~\cite{Swapping}（Swapping）、张量重算~\cite{Recomputation}（Recomputation）等技术来降低推理过程中的显存占用。具体而言，张量交换技术通过张量生命周期分析、异步传输、动态调度等机制，将推理过程中不需要立即使用的张量从GPU内存交换到CPU内存；而张量重算则是将推理过程中的部分中间张量在不需要时释放，将计算图的关键节点保存下来，并在需要时重新计算这些中间结果。然而，简单地将张量交换和张量重算技术应用于LLM推理框架会导致以下两点不足：

首先，张量交换和张量重算技术虽然可以降低推理过程的GPU显存资源占用，但其对LLM推理性能的影响十分复杂，取决于服务器硬件配置（如GPU计算能力）、用户设置（如生成新token时的采样方式）、LLM任务类型与数据集选取、以及推理任务的运行时信息等。已有的LLM推理框架~\cite{Swapping, vLLM, ORCA}虽然已经集成了张量交换或张量重算技术，但其在GPU显存不足时只能固定选择上述技术中的一种，而无法根据上述信息选择更优者，显著影响推理任务的性能。

{\color{red}另外，有关LLM推理优化的工作可以大致分为两类：FasterTransformer~\cite{FasterTransformer}等工作以用户请求为粒度，进行横向调度，这类工作以单请求时延作为优化目标，而忽略了整体吞吐率。ORCA等~\cite{vLLM, ORCA}工作以单次迭代为粒度，进行纵向调度，这类工作以整体吞吐率作为优化目标，而忽略了单请求时延。}整体吞吐率是面向服务器端的性能优化指标，体现了服务器端的处理效率。单请求平均延时是面向客户端的性能优化指标，体现了用户请求处理的实时性。在使用张量交换或张量重算技术优化LLM推理任务显存占用时，需要考虑其对推理任务整体吞吐率和单请求延时的影响，并在二者间进行权衡。

为了解决上述缺陷，本文提出了AdaptiveLLM，一个基于张量交换和张量重算的自适应LLM推理服务框架。该框架实现了针对张量交换和张量重算的精准开销分析，并调用基于开销感知的内存优化策略和基于公平性的用户请求调度策略，在张量交换和张量重算技术间进行动态选择，在降低LLM推理显存占用的同时，降低其对LLM推理吞吐量和单请求时延的影响，进而实现LLM任务的高效推理。
    
\noindent（1）\textbf{本文设计了一款张量重算开销分析器，实现了张量重算开销的精准预测。}本文通过算子粒度计算复杂度分析来识别张量重算开销的影响因素，而后建立回归预测模型来预测单步推理执行时间。实验表明，张量重算开销分析器的预测误差在2\%以内。
    
\noindent（2）\textbf{本文设计了一款张量交换开销分析器，实现了张量交换开销的精准预测。}本文获取用户请求KV Cache的内存占用和GPU-CPU间通信效率信息，来对张量交换的数据传输开销进行预测。实验表明，张量交换开销分析器的预测误差在4\%以内。

\noindent（3）\textbf{本文设计和实现了一个基于张量交换和张量重算的自适应LLM推理服务框架AdaptiveLLM。}该框架引入基于开销感知的内存优化策略，动态选择相应的内存优化技术，通过降低显存占用，来提升推理任务的整体吞吐率。同时引入基于公平性的用户请求调度策略，降低单请求平均延时和带权周转时间。
    
\noindent（4）\textbf{本文选择典型LLM模型和数据集进行全面实验评估。} 在典型LLM模型（OPT~\cite{OPT}、Llama~\cite{Llama}）和数据集（Chatbot~\cite{Chatbot}、Alpaca~\cite{Alpaca}、Summary~\cite{Summary}）上对AdaptiveLLM的张量重算开销分析器、张量交换开销分析器、内存优化决策器、和用户请求调度器模块的有效性进行了实验验证。结果表明，AdaptiveLLM能够实现10\%到40\%的整体吞吐率提升，且将用户请求平均带权周转时间降低20\%至40\%。

\section{引言}

从人脸识别\cite{Face-Recognition}、个性化推荐\cite{Personal-Recommendation}、到智能家居\cite{Smart-Home}、无人驾驶\cite{Self-Driving-Car}等应用领域，深度学习\cite{Deep-Learning}（Deep Learning，DL）相关技术已经融入到社会的方方面面，为人类的生产生活带来了极大的便利。自然语言处理\cite{NLP}（Natural Language Processing，NLP）作为深度学习领域的重要研究方向，长期以来备受人们关注。近年来，随着GPU算力的不断提升，各种语言模型也朝着复杂化，多功能化的方向迅猛发展。 
\par
大语言模型\cite{LLM}（Large Language Models，LLM）是自然语言处理领域的一个分支。LLM通常拥有十亿级别，甚至万亿级别的参数量，因此需要海量的文本数据进行训练。同时，LLM在多种类型的任务中展现出卓越性能，如文本摘要\cite{Text-Summarization}、机器翻译\cite{Machine-Translation}、代码生成\cite{Code-Generation}以及对话问答\cite{Question-Answer}等，拥有巨大的科研价值与商业价值。2021年GPT-3模型\cite{Text-Summarization, GPT3}的问世标志着LLM领域的里程碑，自此，各大科研机构纷纷投入到相关研究中，各种LLM层出不穷，使得该领域的热度空前高涨。 
\par
复杂的结构和爆炸式增长的参数量为LLM带来了卓越的NLP性能，但却为众多的科研工作者们带来了巨大难关。LLM在训练时消耗资源多，花费时间长，失败风险高，性能要求严，使得LLM的训练成本急剧上涨。推理任务的成本相对较低，但极高的内存占用也为推理任务的高效执行带来了独特的挑战。例如，GPT-175B模型仅在权重加载环节就需要消耗325GB的GPU内存空间\cite{GPT-175B资源消耗}，在传统的LLM推理框架下，需要使用至少5个NVIDIA A100 GPU（80GB），且需要引入复杂的并行化策略。因此，降低运行时资源消耗对LLM推理任务至关重要。
\par
复杂的结构和爆炸式增长的参数量为LLM带来了卓越的NLP性能，但却为众多的科研工作者们带来了巨大难关。LLM在训练时消耗资源多，花费时间长，失败风险高，性能要求严，使得LLM的训练成本急剧上涨。推理任务的成本相对较低，但极高的内存占用也为推理任务的高效执行带来了独特的挑战。例如，GPT-175B模型仅在权重加载环节就需要消耗325GB的GPU内存空间 ，在传统的LLM推理框架下，需要使用至少5个NVIDIA A100 GPU（80GB），且需要引入复杂的并行化策略。
\par
为了应对较高参数量带来的GPU内存瓶颈，本文提出了一款基于张量交换\cite{Swapping}（Swapping）和张量重算\cite{Recomputation}（Recomputation）的LLM推理服务框架，引入先进的显存优化策略和用户请求调度策略实现LLM的高效推理。该框架针对服务器端实现高吞吐，针对客户端实现实时请求处理，进一步解决吞吐率与单请求延时在性能优化上长期以来存在的矛盾。 
\par
传统的LLM推理框架拥有诸多可改进之处，下面列举其中最为显著的两点。 
\par
首先，通过张量交换和张量重算等内存优化技术，可以在有限的GPU内存空间中增加批处理大小。然而，上述两项张量优化技术对LLM推理性能的影响十分复杂，取决于服务器硬件环境（如GPU计算能力、GPU-CPU传输带宽）、用户设置（如新token的采样方式）、LLM与数据集选取、以及推理任务的运行时信息等等。已有的推理框架\cite{Swapping, vLLM, ORCA}在面对GPU内存瓶颈时固定调用张量交换或张量重算技术，而无法根据上述信息选择更优者，显著影响推理任务的性能。 
\par
其次、在针对LLM推理任务进行性能优化时，传统工作\cite{Swapping, vLLM, SpecInfer}或者以整体吞吐率为单一导向，或者以单请求延时为单一导向，而没有在二者间进行权衡。吞吐率是面向服务器端的性能优化指标，体现了服务器端的处理效率；单请求平均延时是面向客户端的性能优化指标，体现了用户请求处理的实时性。二者均在LLM应用程序中占有重要地位。
\par
针对传统工作的不足之处，本文设计了AdaptiveLLM,一款基于张量交换和张量重算的LLM推理服务框架。具体而言，本文开展了以下工作：

\begin{itemize} 
    \item { \textbf{本文设计了一款张量重算分析器，实现张量重算开销的精准预测。} 
    \setlength{\parindent}{2em} \par
    通过算子粒度的计算复杂度分析来找出张量重算开销的影响因素，而后模拟LLM的前向传播，收集数据，并建立单步迭代执行时间的回归预测模型。实验表明，张量重算开销的预测误差在2\%以内。}
    \item \textbf{本文设计了一款张量交换分析器，实现张 量交换开销的精准预测。}
    \setlength{\parindent}{2em} \par
    通过获取用户请求KV Cache的内存占用和GPU-CPU间通信效率，来计算数据传输开销。实验证明，张量交换开销预测误差在4\%以内。  
    \item \textbf{本文设计了一个基于张量交换和张量重算的自适应LLM推理加速器。}
    \setlength{\parindent}{2em} \par
    通过引入基于开销感知的张量优化策略，提升整体吞吐率，实现了面向服务器端的处理加速；通过引入基于公平性的用户请求调度策略，降低单请求平均延时和带权周转时间，实现了面向客户端的实时请求处理。
    \item \textbf{本文搭建了一款LLM推理服务框架AdaptiveLLM，并进行实验评估。}
    \setlength{\parindent}{2em} \par
    AdaptiveLLM实现了上述设计的张量重算分析器、张量交换分析器与自适应LLM推理优化器等功能模块。以vLLM框架作为基础程序，在典型LLM（OPT\cite{OPT}、Llama\cite{Llama}）和数据集（Summary\cite{Summary}、Chatbot\cite{Chatbot}、Alpaca\cite{Alpaca}）上进行实验验证。结果表明，本文能够实现10\%到40\%的吞吐率提升，且将用户请求平均带权周转时间降低为60\%至80\%。以此证明本文的优化技术实现了整体吞吐率与单请求延时的权衡，完成了LLM的高效推理。
\end{itemize}
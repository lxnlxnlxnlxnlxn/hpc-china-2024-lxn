\section{结论}

本文设计了AdaptiveLLM，一款基于张量交换和张量重算的LLM推理服务框架。AdaptiveLLM实现了张量重算开销预测与张量交换开销预测，其预测误差分别在2\%和4\%以下。AdaptiveLLM研发了基于开销感知的内存优化策略和基于公平性的用户请求调度策略。基于开销感知的内存优化策略在GPU内存不足时，收集张量交换与张量重算开销预测结果进行比较，执行开销较小的内存优化方式来保证推理任务的顺利完成。基于公平性的用户请求调度策略则在GPU内存充足时，通过计算不同用户请求的优先级，在满足公平性的前提下调度更多的用户请求。实验表明，以vLLM和DeepSpeed框架作为基准程序时，AdaptiveLLM分别实现了$1.3\times\sim2.1\times$和$1.4\times\sim2.3\times$的整体吞吐加速比，实现面向服务器端的处理加速。同时能够以合理的方式调度用户请求，降低平均带权周转时间，缩减比例分别为$20\%\sim40\%$和$25\%\sim65\%$，实现面向客户端的实时处理。综上所述，AdaptiveLLM权衡整体吞吐率与单请求延时，化解二者在优化实现上的矛盾，实现LLM高效推理。
\section{结论}
本文设计了AdaptiveLLM，一款基于张量交换和张量重算的LLM推理服务框架。AdaptiveLLM实现了张量重算开销预测与张量交换开销预测，其预测误差分别在2\%和4\%以下。AdaptiveLLM研发了基于开销感知的张量优化策略和基于公平性的用户请求调度策略。基于开销感知的张量优化策略用于在GPU内存不足时，执行开销较小的抢占方式来保证推理任务的顺利完成；基于公平性的用户请求调度策略则能够在GPU内存充足时重新调度被抢占的用户请求。实验表明，相比于vLLM框架，AdaptiveLLM有10\%-40\%的整体吞吐率提升，实现了服务器端的处理加速；且AdaptiveLLM能够以合理的方式调度用户请求，将平均带权周转时间优化为vLLM的60\%~80\%，减少等待时间，实现了面向客户端的实时请求处理。综上所述，AdaptiveLLM权衡整体吞吐率与单请求延时，化解二者在优化实现上的矛盾。
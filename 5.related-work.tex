\section{相关工作}

%wsq 参考学术paper的相关工作格式，将相关工作分为几类介绍。
\subsubsection{张量交换技术}
%wsq 总分总 参考mimose 中间列举4-5个工作 最后总结 大概写法：之前工作swapping关注activation 本文关注进一步关注到了kvcache并做了精细建模

随着批处理大小的增加或模型参数量的扩展，运行时需要保存的张量会超出GPU的内存限制。张量交换技术在GPU空间不足时开启，将一部分需要保存，而暂时用不到的张量换出至CPU中，在计算需要时重新换入GPU中。

HuggingFace Accelerate~\cite{Huggingface-Accelerate}实现了张量交换技术，但换出与换入的张量仅限于LLM的参数张量。LightLLM~\cite{LightLLM}能够针对KV Cache进行张量交换，但换出的比例设计为定值，无法根据运行时信息调整。

FlexGen~\cite{Swapping}首次提出了“自适应内存优化”的概念，通过线性规划建模在交换方案的可行域内进行搜索，在给定的时间内找到较优解。然而，FlexGen假设运行队列中的所有用户请求拥有相同的输出长度。在实际情况下，输出长度具有很大的差异性，使得相关理论无法推广。

本文针对KV Cache实现张量交换技术，并进行细粒度内存占用建模分析。根据GPU内存使用水平，进行实时换出换入调整。

\subsubsection{张量重算技术}
%同上

Mimose~\cite{Recomputation, Recomp_2, Recomp_3}等工作提出了张量重算技术。在抢占式用户请求调度系统中，当某个请求获得执行权时，会检查之前的计算结果是否保存在GPU中，如果不在，则需要重新获取这部分计算结果。张量重算开销的计算相比与张量交换略微复杂。Capuchin~\cite{Capuchin}将张量重算开销计算过程分解到算子粒度。对于每个算子，通过记录其输入张量与输出张量的生成时间，来获取该算子的重算开销。AdaPipe~\cite{AdaPipe}将连续出现的多个算子组合成计算单元，通过模拟运行来记录各个计算单元的重算开销。

本文针对KV Cache实现张量重算技术。这些key-value张量在初次生成时经历了多次前向传播，而在重算过程中仅需调用自注意力机制即可得到，因此张量重算的开销远远小于token序列初次生成时的开销，不会导致计算量的爆炸式增长。


\subsubsection{LLM推理优化技术}
%结构同上 列举几个工作如orca 最后总结 大概写法：和那些方法正交

除了张量交换和张量重算等针对张量层面的优化策略以外，传统LLM推理框架还采用了很多其他推理优化技术。

ORCA~\cite{ORCA}将批处理调度的粒度从单个用户请求细化为单次推理迭代，化解了用户请求相互等待的性能瓶颈。vLLM~\cite{vLLM}基于OS页式内存管理思想，在ORCA的基础上引入Paged Attention机制。vLLM相比于OCRA，大幅度提升显存利用率，增加批处理大小上限，进而提升推理任务的整体吞吐率。

SpecInfer~\cite{SpecInfer}引入了投机推理技术（Speculative Sampling），根据小型LLM的输出来预测大型LLM的输出，在大幅度提升推理吞吐率的同时保障了输出质量。DistillSpec~\cite{DistillSpec}在SpecInfer的基础上实现了知识蒸馏技术（Knowledge Distillation，KD），使得输出预测的准确率显著提升。

本文提出的LLM推理优化策略能够与调度粒度细化、投机推理等研究工作相兼容。本文设计了规范且友好的用户接口，开发者能够根据任务需求来定义各种参数，极大地方便了有关推理优化的深入研究。

% \subsubsection{\color{red}{早期工作}}

% Hugging Face Accelerate~\cite{Huggingface-Accelerate}实现了张量交换技术，但换出与换入的张量仅限于LLM的参数张量。DeepSpeed ZeRO-Inference~\cite{GPT-175B资源消耗}实现了LLM的分布式推理，能够利用数据并行性与张量并行性来实现LLM推理加速。但以上两个框架均无法针对KV Cache实现张量交换技术，也没有实现张量重算。
  
% \subsubsection{\color{red}{张量交换的提出}}

% FlexGen~\cite{Swapping}首次提出了“自适应内存优化”的概念，并将张量交换的范围由参数张量扩展至所有张量。通过线性规划建模在交换方案的可行域内进行搜索，在给定的时间内找到较优解。同时，FlexGen还实现了张量压缩技术，相比于Hugging Face Accelerate和DeepSpeed ZeRO-Inference，实现了较大的吞吐率提升。然而，FlexGen假设运行队列中的所有用户请求拥有相同的输出长度。在实际情况下，输出长度具有很大的差异性，使得相关理论无法推广。
  
% \subsubsection{\color{red}{调度粒度的转变}}

% ORCA~\cite{ORCA}将批处理调度的粒度从单个用户请求转化为单次推理迭代，化解了用户请求相互等待的性能瓶颈。vLLM~\cite{vLLM}在ORCA的基础上实现了张量重算技术。基于OS页式内存管理思想，引入Paged Attention机制来实现。vLLM相比于OCRA，大幅度提升显存利用率，并增加批处理大小上限，进而提升推理任务的整体吞吐率。同时，vLLM设计了规范且友好的用户接口，开发者能够根据任务需求来定义各种参数，极大地方便了有关推理优化的深入研究。
  
% \subsubsection{\color{red}{投机推理技术}}

% SpecInfer~\cite{SpecInfer}引入了投机推理技术（Speculative Sampling），根据小型LLM的输出来预测大型LLM的输出，在大幅度提升推理吞吐率的同时保障了输出质量。DistillSpec~\cite{DistillSpec}在SpecInfer的基础上实现了知识蒸馏技术（Knowledge Distillation，KD），使得输出预测准确率显著提升。

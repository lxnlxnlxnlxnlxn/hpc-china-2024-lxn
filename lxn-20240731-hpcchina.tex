%! TeX program=latexmk
%! TeX options=-xelatex -synctex=1 -interaction=nonstopmode -file-line-error "%DOC%"
\documentclass[nosysfonts, a4paper]{hpcchina}

\usepackage{graphicx}
\usepackage{amsmath,amsthm}
\usepackage{amssymb,amsfonts}
%以下宏包为测试用途
\usepackage{blindtext}
\usepackage{zhlipsum}
\usepackage{tikz}
\usepackage{metalogo}

%LXN
\graphicspath{ {pictures/} }  %使用图片
\usepackage{algorithm}  % 伪代码编辑
\usepackage{algorithmic}  % 伪代码编辑
\renewcommand{\algorithmicrequire}{\textbf{Input:}}  % 伪代码输入
\renewcommand{\algorithmicensure}{\textbf{Output:}}  % 伪代码输出
\usepackage{setspace}  % 伪代码行距
\usepackage{booktabs}
\usepackage[UTF8]{ctex}

\tracinglostchars=2

%标题
\title{AdaptiveLLM：基于自适应张量交换和张量重算的大语言模型推理优化}

%作者
\author{
  梁绪宁\textsuperscript{1,*}，
  王思琪\textsuperscript{1,*}，
  杨海龙\textsuperscript{1}，
  栾钟治\textsuperscript{1}，
  刘轶\textsuperscript{1}，
  钱德沛\textsuperscript{1}
}
%单位
\affiliation{
  \textsuperscript{1} 北京航空航天大学，北京 100191
}
%邮箱，使用\mailurl{地址}可以生成可点击的链接
\email{
  \mailurl{
    liangxuning@126.com,
    lethean1@buaa.edu.cn,
    hailong.yang@buaa.edu.cn,
    07680@buaa.edu.cn,
    yi.liu@buaa.edu.cn,
    depeiq@buaa.edu.cn
  }
}

%中文摘要
\cabstract{
  大语言模型（LLMs）拥有极高的参数量，为推理任务带来GPU内存瓶颈。已有LLM推理框架引入张量交换和张量重算等内存优化技术，在有限的GPU内存上通过牺牲性能完成推理。然而，已有工作无法根据推理任务运行时信息自适应地选择内存优化技术，导致推理任务的性能无法进一步提升。同时，这些工作大都以推理任务整体吞吐率为单一优化目标，缺乏对用户请求调度的公平性考虑。针对以上问题，本文面向LLM推理服务场景，提出了AdaptiveLLM，一款基于自适应张量交换和张量重算的LLM推理服务框架。AdaptiveLLM实现了张量重算和张量交换开销精准预测，其预测误差分别控制在2\%和4\%以下。在此基础上，引入基于开销感知的内存优化策略，可以自适应地选择开销较低的内存优化技术，提高任务整体吞吐率。同时，引入基于公平性的用户请求调度策略，降低单请求延时。本文在主流LLM模型和数据集上开展实验验证，以vLLM和DeepSpeed作为基准程序进行对比评估。结果表明，AdaptiveLLM分别实现了$1.3\times\sim2.1\times$和$1.4\times\sim2.3\times$的整体吞吐率提升，同时使平均带权周转时间降低，压缩比例分别为$20\%\sim50\%$和$25\%\sim65\%$。由此证明AdaptiveLLM在推理优化过程中可以更好地权衡整体吞吐率与用户请求调度公平性，实现LLM高效推理。
}
%中文关键词
\keyword{
  大语言模型；推理；张量交换；张量重算
}

%英文标题
\etitle{AdaptiveLLM: Efficient LLM inference using adaptive tensor swapping and re-computation techniques}
%英文作者
\eauthor{
  Liang Xuning\textsuperscript{1}，
  Wang Siqi\textsuperscript{1}，
  Yang Hailong\textsuperscript{1}，
  Luan Zhongzhi\textsuperscript{1}，
  Liu Yi\textsuperscript{1}，
  Qian Depei\textsuperscript{1}
}
%英文单位
\eaffiliation{
  \textsuperscript{1} (Beihang University, Beijing 100191)
}
%英文摘要
\eabstract{
  Large Language Models(LLMs) come with an extremely high amount of parameters, posing significant challenges on inference tasks. Traditional LLM inference services employ tensor swapping and tensor re-computation techniques, guaranteeing the success of generation at the cost of performance on limited GPU memory. However, existing LLM serving systems fail to search memory management schemes adaptively based on runtime information, leading to a sub-optimal performance. Furthermore, these works focus exclusively on the throughput of the inference tasks, lacking consideration for scheduling fairness. To address the above issues, we propose \textit{AdaptiveLLM}, an efficient LLM serving framework for inference tasks based on swapping and re-computation. Specifically, \textit{AdaptiveLLM} implements an overhead predictor for swapping and re-computation, with an error rate lower than 2\% and 4\% respectively. \textit{AdaptiveLLM} also adopts a cost-aware memory optimization algorithm which improves throughput on the server, and a fairness-based request scheduling algorithm which reduces the proportion of waiting time for the client. On typical LLMs and datasets, \textit{AdaptiveLLM} achieves a speedup by $1.3\times\sim2.1\times$ and $1.4\times\sim2.3\times$, while reduces the average weighted around time by $20\%\sim50\%$ and $25\%\sim65\%$, compared with the \textit{vLLM} and \textit{DeepSpeed} baseline. In conclusion, \textit{AdaptiveLLM} achieves efficient LLM inference by making a better trade off between throughput and fairness.
}
%英文关键词
\ekeyword{
  LLM; Inference; Tensor Swapping; Tensor Re-computation
}

%基金号
\grants{国家重点研发项目(2023YFB3001801), 自然科学基金项目(62322201, 62072018, U23B2020, U22A2028), 中央高校基本科研业务费专项资金资助(YWF-23-L-1121, JKF-20240198), 复杂软件全国重点实验室(SKLSDE-2023ZX-05) \\ 通信作者：杨海龙}
%DOI号
\doi{missing DOI}
%分类号
\clcls{TP391}
%卷(期):起止页,年
\issue{卷(期):起止页,年}
%收稿日期
\dateaccept{2024-07-31}
%修回日期
\daterevise{2024-08-31}

\begin{document}
  \maketitle
  \input{1.introduction.tex}
  \input{2.background}
  % \input{3.motivation}
  \input{3.related-work}
  \input{4.design}
  \input{5.evaluation}
  \input{6.future-work}
  \input{7.conclusion}
  \bibliography{references.bib}
\end{document}
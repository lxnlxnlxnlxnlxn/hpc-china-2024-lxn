%! TeX program=latexmk
%! TeX options=-xelatex -synctex=1 -interaction=nonstopmode -file-line-error "%DOC%"
\documentclass[nosysfonts, a4paper]{hpcchina}

\usepackage{graphicx}
\usepackage{amsmath,amsthm}
\usepackage{amssymb,amsfonts}
%以下宏包为测试用途
\usepackage{blindtext}
\usepackage{zhlipsum}
\usepackage{tikz}
\usepackage{metalogo}

%LXN
\graphicspath{ {pictures/} }  %使用图片
\usepackage{algorithm}  % 伪代码编辑
\usepackage{algorithmic}  % 伪代码编辑
\renewcommand{\algorithmicrequire}{\textbf{Input:}}  % 伪代码输入
\renewcommand{\algorithmicensure}{\textbf{Output:}}  % 伪代码输出
\usepackage{setspace}  % 伪代码行距
\usepackage{booktabs}
\usepackage[UTF8]{ctex}

\tracinglostchars=2

%标题
\title{AdaptiveLLM：基于自适应张量交换和张量重算的大语言模型推理优化}

%作者
\author{
  梁绪宁\textsuperscript{1}，
  王思琪\textsuperscript{1}，
  杨海龙\textsuperscript{1}，
  栾钟治\textsuperscript{1}，
  刘轶\textsuperscript{1}，
  钱德沛\textsuperscript{1}
}
%单位
\affiliation{
  \textsuperscript{1} 北京航空航天大学，北京 100191
}
%邮箱，使用\mailurl{地址}可以生成可点击的链接
\email{
  \mailurl{
    liangxuning@126.com,
    lethean1@buaa.edu.cn,
    hailong.yang@buaa.edu.cn,
    07680@buaa.edu.cn,
    yi.liu@buaa.edu.cn,
    depeiq@buaa.edu.cn
  }
}

%中文摘要
\cabstract{
  大语言模型（LLMs）拥有极高的参数量，为推理任务带来 GPU 内存瓶颈。已有LLM推理框架引入张量交换和张量重算等内存优化技术，在有限的GPU内存上通过牺牲性能完成推理。然而，已有工作无法根据推理任务运行时信息自适应地选择内存优化技术，导致推理任务的性能无法进一步提升。同时，这些工作以推理任务整体吞吐率或单请求响应延时为单一优化目标，缺乏对上述优化目标的综合考虑。针对以上问题，本文面向LLM推理服务场景，提出了AdaptiveLLM，一款基于自适应张量交换和张量重算的LLM推理框架。AdaptiveLLM实现了张量重算和张量交换开销精准预测，其预测误差分别控制在2\%和4\%以下。在此基础上，引入了基于开销感知的内存优化策略，可以自适应地选择开销较低的内存优化技术，提高了任务整体吞吐率。同时，引入了基于公平性的用户请求调度策略，降低了单请求延时。本文在主流LLM模型和数据集上开展实验验证，以vLLM作为基准程序进行对比评估。结果表明，AdaptiveLLM实现了10\%到40\%的整体吞吐率提升，同时使平均带权周转时间降低了20\%至40\%。由此证明AdaptiveLLM在推理优化过程中可以更好地权衡整体吞吐率与单请求延时，实现LLM高效推理。
}
%中文关键词
\keyword{
  大语言模型；推理；张量交换；张量重算；自适应内存优化
}

%英文标题
\etitle{AdaptiveLLM: Efficient LLM inference using adaptive tensor swapping and re-computation techniques}
%英文作者
\eauthor{
  Liang Xuning\textsuperscript{1}，
  Wang Siqi\textsuperscript{1}，
  Yang Hailong\textsuperscript{1}，
  Luan Zhongzhi\textsuperscript{1}，
  Liu Yi\textsuperscript{1}，
  Qian Depei\textsuperscript{1}
}
%英文单位
\eaffiliation{
  \textsuperscript{1} (Beihang University, Beijing 100191)
}
%英文摘要
\eabstract{
  Large Language Models(LLMs) come with an extremely high amount of parameters, posing significant challenges for inference tasks. Traditional LLM inference services employ swapping and re-computation techniques, guaranteeing the success of generation at the cost of performance on limited GPU memory. However, existing LLM serving systems fail to search memory management schemes adaptively based on runtime information, leading to a sub-optimal performance. Furthermore, these works are inferior in the trade-off between throughput and latency, preferring on only one and compromising the other. To address the above issues, we propose \textit{AdaptiveLLM}, an efficient LLM serving framework for inference tasks based on swapping and re-computation. Specifically, \textit{AdaptiveLLM} implements an overhead predictor for swapping and re-computation, with an error rate lower than 2\% and 4\% respectively. \textit{AdaptiveLLM} also adopts a cost-aware memory optimization algorithm which improves throughput on the server, and a fairness-based request scheduling algorithm which reduces latency for the client. On typical LLMs and datasets, our evaluation shows that \textit{AdaptiveLLM} improves the throughput by 10\% to 40\%, and reduces the average weighted around time by 20\% to 40\%, compared with the vLLM. In conclusion, \textit{AdaptiveLLM} achieves efficient LLM inference by making a better trade off between throughput and latency.
}
%英文关键词
\ekeyword{
  LLM; Inference; Tensor Swapping; Tensor Re-computation; Adaptive Memory Optimization
}

%基金号
\grants{目前无人捐助此项目}
%DOI号
\doi{missing DOI}
%分类号
\clcls{TP391}
%卷(期):起止页,年
\issue{卷(期):起止页,年}
%收稿日期
\dateaccept{2024-07-31}
%修回日期
\daterevise{2024-08-31}

\begin{document}
  \maketitle
  \input{1.introduction.tex}
  \input{2.background}
  % \input{3.motivation}
  \input{3.design}
  \input{4.evaluation}
  \input{5.related-work}
  \input{6.future-work}
  \input{7.conclusion}
  \bibliography{references.bib}
\end{document}
%! TeX program=latexmk
%! TeX options=-xelatex -synctex=1 -interaction=nonstopmode -file-line-error "%DOC%"
\documentclass[nosysfonts, a4paper]{hpcchina}

\usepackage{graphicx}
\usepackage{amsmath,amsthm}
\usepackage{amssymb,amsfonts}
%以下宏包为测试用途
\usepackage{blindtext}
\usepackage{zhlipsum}
\usepackage{tikz}
\usepackage{metalogo}

%LXN
\graphicspath{ {pictures/} }  %使用图片
\usepackage{algorithm}  % 伪代码编辑
\usepackage{algorithmic}  % 伪代码编辑
\renewcommand{\algorithmicrequire}{\textbf{Input:}}  % 伪代码输入
\renewcommand{\algorithmicensure}{\textbf{Output:}}  % 伪代码输出
\usepackage{setspace}  % 伪代码行距
\usepackage{booktabs}
\usepackage[UTF8]{ctex}

\tracinglostchars=2

%标题
\title{AdaptiveLLM:基于张量交换和张量重算的大语言模型推理优化技术}

%作者
\author{段晓辉\textsuperscript{1}}
%单位
\affiliation{
  \textsuperscript{1} 清华大学，北京 100084
}
%邮箱，使用\mailurl{地址}可以生成可点击的链接
\email{\mailurl{sunrise_duan@126.com}}

%中文摘要
%wsq 什么叫张量优化策略？应该是内存优化策略？全文修改
\cabstract{
  大语言模型（LLMs）拥有极高的参数量，为推理任务带来GPU内存瓶颈。传统LLM推理框架引入张量交换和张量重算等技术，在有限的GPU内存上牺牲性能完成推理。然而，已有研究工作无法根据推理任务运行时信息自适应地选择内存优化技术，导致推理任务的性能无法进一步提升。同时这些工作没有实现整体吞吐率与单请求延时之间的权衡，常以二者之一作为优化目标。针对以上问题，本文面向大模型推理服务场景，提出AdaptiveLLM，一款基于张量交换和张量重算的LLM推理框架。AdaptiveLLM实现了张量重算和张量交换开销预测，其预测误差分别在2\%和4\%以下；引入基于开销感知的内存优化策略，实现服务器端处理加速；引入基于公平性的用户请求调度策略，实现客户端实时请求响应。本文在常见LLM和数据集上开展实验，以vLLM作为基准程序进行对比评估。结果表明，基于开销感知的内存优化策略带来1.1到1.4的整体吞吐加速比；基于公平性的用户请求调度策略使平均带权周转时间降低20\%至40\%。由此证明AdaptiveLLM在优化过程中权衡整体吞吐率与单请求延时，实现LLM高效推理。
}
%中文关键词
\keyword{
  大语言模型、推理、张量交换、张量重算
}

%英文标题
\etitle{AdaptiveLLM: Efficient LLM inference based on swapping and re-computation}
%英文作者
\eauthor{Xiaohui Duan\textsuperscript{1}}
%英文单位
\eaffiliation{
  \textsuperscript{1} (Tsinghua University, Beijing 100084)
}
%英文摘要
% wsq英文部分对照上面修改
\eabstract{
  Large Language Models (LLMs) come with an extremely high amount of parameters, posing significant challenges for inference tasks. Traditional LLM inference services employ swapping and re-computation techniques, guaranteeing the success of generation at the cost of performance on limited GPU memory. However, existing LLM serving systems fail to search memory management schemes adaptively based on runtime information, leading to a sub-optimal performance. Furthermore, these works are inferior in the trade-off between throughput and latency, preferring on only one and compromising the other. To address the above issues, we propose AdaptiveLLM, an efficient LLM service for inference tasks based on swapping and re-computation. We implements an overhead predictor for swapping and re-computation, with an error rate lower than 2\% and 4\% respectively. AdaptiveLLM adopts a cost-aware memory optimization algorithm and a fairness-based request scheduling algorithm based on the overhead predictor. The former improves the throughput on the server, and the latter reduces latency for the client, making a enhancement on the real-time performance. On typical LLMs and datasets, our evaluation shows that the cost-aware memory optimization algorithm improves the throughput by 10\% to 40\%, and the fairness-based request scheduling algorithm reduces the average weighted around time by 20\% to 40\%, compared with the vLLM baseline. In conclusion, AdaptiveLLM achieves efficient LLM inference by making a better trade off between throughput and latency.
}
%英文关键词
\ekeyword{
  LLM, inference, swapping, re-computation
}

%基金号
\grants{目前无人捐助此项目}
%DOI号
\doi{missing DOI}
%分类号
\clcls{TP391}
%卷(期):起止页,年
\issue{卷(期):起止页,年}
%收稿日期
\dateaccept{2024-07-31}
%修回日期
\daterevise{2024-08-31}

\begin{document}
  \maketitle
  \input{1.introduction.tex}
  \input{2.background}
  \input{3.design}
  \input{4.evaluation}
  \input{5.related-work}
  \input{6.future-work}
  \input{7.conclusion}
  \bibliographystyle{gbt7714-numerical}
  \bibliography{references}
\end{document}